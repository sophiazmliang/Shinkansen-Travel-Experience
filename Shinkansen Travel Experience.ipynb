{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sophiazmliang/hackathon-shinkansen-travel-experience?scriptVersionId=191181773\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Shinkansen Travel Experience**\n\n## **Context:**\n\nThis problem statement is based on the Shinkansen Bullet Train in Japan, and passengers’ experience with that mode of travel. This machine-learning exercise aims to determine the relative importance of each parameter with regard to their contribution to the passengers’ overall travel experience. The dataset contains a random sample of individuals who traveled on this train. The on-time performance of the trains along with passenger information is published in a file named ‘Traveldata_train.csv’.  These passengers were later asked to provide their feedback on various parameters related to the travel along with their overall experience. These collected details are made available in the survey report labeled ‘**Surveydata_train.csv**’.\n\nIn the survey, each passenger was explicitly asked whether they were satisfied with their overall travel experience or not, and that is captured in the data of the survey report under the variable labeled ‘**Overall_Experience**’.\n\n\n## **Objective:**\n\n The objective of this problem is to understand which parameters play an important role in swaying passenger feedback towards a positive scale. You are provided test data containing the travel data and the survey data of passengers. Both the test data and the train data are collected at the same time and belong to the same population.\n \nI also documented my journey, challenges, and learnings during this Hackathon in an article on Medium. \n\nYou can read the full article here. https://medium.com/@sophiazmliang/accelerating-data-science-skills-my-hackathon-journey-with-chatgpt-6ced31215a5b\n\nI’d love to hear your thoughts, feedback, and any similar experiences you’ve had. Feel free to leave a comment or ask any questions. Let's learn and grow together! Happy Kaggling!\n\n\n## **Problem Statement**\n\n**Goal:**\nThe goal of the problem is to **predict whether a passenger was satisfied or not** considering his/her overall experience of traveling on the Shinkansen Bullet Train.\n\n**Dataset:**\n\nThe problem consists of 2 separate datasets: **Travel data & Survey data. Travel data** has information related to passengers and attributes related to the Shinkansen train, in which they traveled. The survey data is aggregated data of surveys indicating the post-service experience. You are expected to treat both these datasets as raw data and perform any necessary data cleaning/validation steps as required.\n\nThe data has been split into two groups and provided in the Dataset folder. The folder contains both train and test data separately.\n\n- Train_Data\n- Test_Data\n\n**Target Variable**: Overall_Experience (1 represents ‘satisfied’, and 0 represents ‘not satisfied’)\n\nThe **training set** can be used to build your machine-learning model. The training set has labels for the target column - **Overall_Experience**.\n\nThe **testing set** should be used to see how well your model performs on unseen data. For the test set, it is expected to predict the ‘**Overall_Experience**’ level for each participant.\n\n**Data Dictionary:**\n\nAll the data is self-explanatory. The survey levels are explained in the Data Dictionary file.\n\n**Submission File Format:** You will need to submit a CSV file with exactly 35,602 entries plus a header row. The file should have exactly two columns\n\n- ID\n- **Overall_Experience** (contains 0 & 1 values, 1 represents ‘Satisfied’, and 0 represents ‘Not Satisfied’)\n\n**Evaluation Criteria:**\n\n**Accuracy Score:** The evaluation metric is simply the percentage of predictions made by the model that turned out to be correct. This is also called the accuracy of the model. It will be calculated as the total number of correct predictions (True Positives + True Negatives) divided by the total number of observations in the dataset.\n\nIn other words, the best possible accuracy is 100% (or 1), and the worst possible accuracy is 0%.\n\n\n","metadata":{"id":"OWDEjuizESEl"}},{"cell_type":"markdown","source":"### **Importing Libraries and the Dataset**","metadata":{"id":"NRJtXkTrHxMQ"}},{"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nfrom numpy import mean\nfrom numpy import std\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# Algorithms to use\nfrom sklearn import tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Metrics to evaluate the model\nfrom sklearn import metrics\n\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# For tuning the model\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"id":"568c25ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install hyperopt\n!pip install catboost\n!pip install lightgbm\n","metadata":{"id":"wCDqtxlugRjg","executionInfo":{"status":"ok","timestamp":1721687324261,"user_tz":240,"elapsed":47427,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"49b57425-404c-49bf-ad1d-62ff0f4d3221"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom hyperopt import fmin, tpe, hp, Trials","metadata":{"id":"mXnBGWB1bzCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","metadata":{"id":"z4bHewRri74f","executionInfo":{"status":"ok","timestamp":1719465895141,"user_tz":240,"elapsed":13,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"6c018e22-8bd1-4ba7-b6cb-85dedc37dbe7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !nvidia-smi","metadata":{"id":"ZccNb6KujAG3","executionInfo":{"status":"ok","timestamp":1719465895305,"user_tz":240,"elapsed":173,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"8802702e-51c9-469f-eb53-3472e7faa23c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount(\"/content/drive\", force_remount=True)","metadata":{"id":"V6eSS6r77Im-","executionInfo":{"status":"ok","timestamp":1721687351274,"user_tz":240,"elapsed":20652,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a0213522-1260-4096-e580-e1da75098af6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_train = pd.read_csv('/content/drive/MyDrive/Colab/Shinkansen Travel Experience/Traveldata_train_(2).csv')\ns_train = pd.read_csv('/content/drive/MyDrive/Colab/Shinkansen Travel Experience/Surveydata_train_(2).csv')\nt_test = pd.read_csv('/content/drive/MyDrive/Colab/Shinkansen Travel Experience/Traveldata_test_(2).csv')\ns_test = pd.read_csv('/content/drive/MyDrive/Colab/Shinkansen Travel Experience/Surveydata_test_(2).csv')","metadata":{"id":"82b2eab4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Understanding the data**","metadata":{"id":"12TKB2M7XyC6"}},{"cell_type":"code","source":"df_list=[t_train, s_train, t_test, s_test]","metadata":{"id":"c250415f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\npd.set_option('display.width', 1000)\npd.set_option('display.max_columns', None)","metadata":{"id":"vRzW_Wirk7Ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_list:\n    display(i.head(5))","metadata":{"id":"mLc8Hlknzg68","executionInfo":{"status":"ok","timestamp":1721687717961,"user_tz":240,"elapsed":608,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"d998c97a-cb6d-47d9-c469-5ed91e63f907"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_list:\n    i.info()","metadata":{"id":"3b023f57","executionInfo":{"status":"ok","timestamp":1721687864047,"user_tz":240,"elapsed":260,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"1a9a729b-9940-46cb-875a-379d25720592"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_list:\n    print(i.nunique())","metadata":{"id":"070829c4","executionInfo":{"status":"ok","timestamp":1721687397378,"user_tz":240,"elapsed":352,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"5f77194c-5304-47a6-f991-f76655ae78ff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Preprocessing**\n\n\n\n*   Data Merging\n*   Checking for missing values\n*   Checking for duplicate values\n\n","metadata":{"id":"rhOl-x234TP8"}},{"cell_type":"code","source":"# Merging Training Data\ndf_train = pd.merge(t_train, s_train, how = 'left', on ='ID')\ndf_train.shape","metadata":{"id":"e4e81d8d","executionInfo":{"status":"ok","timestamp":1721687992316,"user_tz":240,"elapsed":180,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"ab8ec4e1-20f2-4496-e3e8-7cc1e2882d78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are duplicate entries\ndf_train.duplicated().sum()","metadata":{"id":"delN6PC0vI-B","executionInfo":{"status":"ok","timestamp":1721687995393,"user_tz":240,"elapsed":360,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"dccb43ce-4932-406c-c56d-11245cd0018b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"id":"XfdxwSu-p6_B","executionInfo":{"status":"ok","timestamp":1721688023296,"user_tz":240,"elapsed":179,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"c1f14fea-f5b5-4d76-8ccf-30068920b78f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"id":"dSFFCNuu2BOg","executionInfo":{"status":"ok","timestamp":1721688026981,"user_tz":240,"elapsed":149,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"5cc5ecf2-2d7f-4514-a1e2-a1cc3089baa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MERGE Test data\ndf_test = pd.merge(t_test,s_test, how = 'left', on ='ID')\ndf_test.shape","metadata":{"id":"f5644924","executionInfo":{"status":"ok","timestamp":1721688032827,"user_tz":240,"elapsed":181,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"073a524c-bc23-43d6-8c31-8bb6667a856a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"id":"WHjO02cjqOyB","executionInfo":{"status":"ok","timestamp":1721688037204,"user_tz":240,"elapsed":173,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a54c7e4e-a9b3-432b-c80d-78baf6371691"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"id":"OS3XIrqG2ZHC","executionInfo":{"status":"ok","timestamp":1721688050350,"user_tz":240,"elapsed":161,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"4cd0bd41-44ac-4f0a-8b6e-3f1938f75d08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if there are duplicate entries\ndf_test.duplicated().sum()","metadata":{"id":"Mf7REOQnvDAE","executionInfo":{"status":"ok","timestamp":1721688052710,"user_tz":240,"elapsed":197,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"fa8e5ea7-80d2-4c10-f8af-edcf70551013"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[df_test['ID'].isin(df_train['ID'])]","metadata":{"id":"2nsXKlzkpQ6-","executionInfo":{"status":"ok","timestamp":1721688056760,"user_tz":240,"elapsed":157,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3924509b-ed63-4840-a0fe-eb387cc66d8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['ID'].isin(df_test['ID'])]","metadata":{"id":"MqwLLWw9qHp8","executionInfo":{"status":"ok","timestamp":1721688058031,"user_tz":240,"elapsed":181,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"296f0ac8-cc9b-485a-b40f-5b56ef74ec88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df_train, df_test], axis=0)","metadata":{"id":"c842fe4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.set_index('ID')","metadata":{"id":"M1uett1HsUGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"oXX96BC1ov1R","executionInfo":{"status":"ok","timestamp":1721688069173,"user_tz":240,"elapsed":372,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"2d31c854-9b2e-46db-f475-b0c287d92afb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"oHa9lhBHvWy_","executionInfo":{"status":"ok","timestamp":1721688074291,"user_tz":240,"elapsed":169,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"1035bcd2-8bc1-41d7-8080-0afb10fa637a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_backup = df.copy(deep=True)","metadata":{"id":"317j_WNrwg-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data Imputation**","metadata":{"id":"ON87v-X_m-jy"}},{"cell_type":"code","source":"#df = df_backup.copy(deep=True)","metadata":{"id":"AJ7VloPou2ia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"_9084gDEwG3p","executionInfo":{"status":"ok","timestamp":1721688377696,"user_tz":240,"elapsed":188,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"c8e5c1da-e823-49cb-c4c8-684b9ae02bd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = df.select_dtypes(include='number').columns","metadata":{"id":"FnV-T91YY8Kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols","metadata":{"id":"El2tmQGwY95V","executionInfo":{"status":"ok","timestamp":1721688399043,"user_tz":240,"elapsed":254,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"9c11bde0-616e-40b2-833b-0a841dfa4965"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = df.select_dtypes(include='object').columns","metadata":{"id":"jPo1LPNZZnGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols","metadata":{"id":"CL6HXULJZvM5","executionInfo":{"status":"ok","timestamp":1721688405065,"user_tz":240,"elapsed":253,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"ded4db27-ded8-4808-c7e9-3e9a3307c6a0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking summary statistics\ndf[num_cols].describe().T","metadata":{"id":"-jKGxSfVZN9j","executionInfo":{"status":"ok","timestamp":1721688416188,"user_tz":240,"elapsed":198,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"abb444a1-efa7-40c9-8a63-30008cc46c91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # use departure delay to fill null arrival delay\n# df.Arrival_Delay_in_Mins.fillna(df.Departure_Delay_in_Mins, inplace=True)","metadata":{"id":"221f3bc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Arrival_Delay_in_Mins.fillna(0, inplace=True)\ndf.Departure_Delay_in_Mins.fillna(0, inplace=True)","metadata":{"id":"m8_4QnMd9NTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Replace Missing values with mode for Columns with Little number of Missing Values\n# clean_columns = ['Gender', 'Age', 'Seat_Comfort', 'Platform_Location',\n#                  'Onboard_Wifi_Service', 'Onboard_Entertainment', 'Online_Support',\n#                  'Ease_of_Online_Booking', 'Legroom', 'Baggage_Handling',\n#                  'CheckIn_Service', 'Cleanliness', 'Online_Boarding']\n# for col in clean_columns:\n#     mode_value = df[col].mode()[0]\n#     df[col].fillna(mode_value, inplace=True)","metadata":{"id":"e1fed06c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loop through the categorical columns\n# for col in cat_cols:\n#     print(f\"Column: {col}\")\n#     value_counts = df[col].value_counts()\n#     print(f\"Unique Value Distribution:\\n{value_counts}\\n\")","metadata":{"id":"RmlxmHmIaUKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace Missing values with mode for Columns with More number of Missing Values\n#clean_columns2 = ['Customer_Type', 'Type_Travel', 'Arrival_Time_Convenient', 'Catering',\n#                 'Onboard_Service']\n#for col in clean_columns2:\n#    df[col].fillna('Unknown', inplace=True)","metadata":{"id":"Em2gR0g4_6tI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"4DyYwvgI9TDs","executionInfo":{"status":"ok","timestamp":1721688543532,"user_tz":240,"elapsed":183,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"f58f2e28-f5d1-484c-fe02-f2c32cecb362"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"3LSqlvoC9bcN","executionInfo":{"status":"ok","timestamp":1721688566581,"user_tz":240,"elapsed":252,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"588a06c8-f394-470f-ef94-1cf1b38c0dce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{"id":"52aa1ed6"}},{"cell_type":"code","source":"for col in num_cols:\n    print(col)\n    print('Skew :',round(df[col].skew(),2))\n    plt.figure(figsize=(15,4))\n    plt.subplot(1,2,1)\n    df[col].hist(bins=30)\n    plt.ylabel('count')\n    plt.subplot(1,2,2)\n    sns.boxplot(x=col, data = df)\n    plt.show()","metadata":{"id":"3cPhIae15xA9","executionInfo":{"status":"ok","timestamp":1721688610439,"user_tz":240,"elapsed":4145,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"22e27f41-9bc8-4357-b621-e80feb593e7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[num_cols].describe().T","metadata":{"id":"Wj9obuk88xMV","executionInfo":{"status":"ok","timestamp":1721688642434,"user_tz":240,"elapsed":225,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"36221090-6b6c-4518-c407-9ab17e6f1511"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of columns to be plotted\nnum_of_cat_cols = len(cat_cols)\n\n# Determine the number of rows and columns for the subplots\nn_cols = 4  # Number of columns in the grid\nn_rows = (num_of_cat_cols // n_cols) + (num_of_cat_cols % n_cols > 0)  # Calculate rows needed\n\n# Create subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 15))\naxes = axes.flatten()  # Flatten the array of axes for easy iteration\n\n# Loop through the categorical columns and create a bar plot for each\nfor i, col in enumerate(cat_cols):\n    sns.countplot(data=df, x=col, palette='viridis', ax=axes[i])\n    axes[i].set_title(f'Bar Plot of {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=90)\n\n# Remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()","metadata":{"id":"QZ8YL5Kgr7te","executionInfo":{"status":"ok","timestamp":1721688657193,"user_tz":240,"elapsed":9597,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"ea25d259-0f98-4e81-f598-8a096008df40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding Categorical Values.\ndf = df.replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'],\n           [6, 5, 4, 3, 2, 1])\ndf = df.replace(['Male', 'Female'], [1,0])\ndf = df.replace(['Loyal Customer', 'Disloyal Customer'], [1,0])\ndf = df.replace(['Business Travel', 'Personal Travel'], [1,0])\ndf = df.replace(['Business', 'Eco'], [1,0])\ndf = df.replace(['Green Car', 'Ordinary'], [1,0])\ndf = df.replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'],\n           [6, 5, 4, 3, 2, 1])","metadata":{"id":"bJlnWgf8BkBq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude\nexclude_cols = pd.Index(['Gender', 'Customer_Type', 'Type_Travel', 'Travel_Class', 'Seat_Class'])\n\n# Define new Index excluding specified columns\ncat_6_rating_cols = cat_cols.difference(exclude_cols)\n\nprint(cat_6_rating_cols)","metadata":{"id":"91UVdQaRqbg0","executionInfo":{"status":"ok","timestamp":1721688757965,"user_tz":240,"elapsed":171,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"8e2d3a6e-6999-402f-d520-0c82ce121f56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to perform the cumulative distribution-based transformation\ndef transform_column(df, column):\n    value_counts = df[column].value_counts(normalize=True)\n    cumulative_distribution = value_counts.sort_index().cumsum()\n    rating_dict = cumulative_distribution * 6\n    return df[column].map(rating_dict)\n\n# Loop through each column in cat_6_rating_cols and transform it\nfor col in cat_6_rating_cols:\n    transformed_col = transform_column(df, col)\n    df[col + '_rating'] = transformed_col","metadata":{"id":"P6-D91nXrOSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"uhp46CHnrjXl","executionInfo":{"status":"ok","timestamp":1721688928497,"user_tz":240,"elapsed":171,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"f17462b1-8213-4f68-f752-dc9bc5c7753f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=cat_6_rating_cols, inplace=True)","metadata":{"id":"DMRQCfi3sI_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define the ordered responses\n# ordered_responses = [1, 2, 3, 4, 5, 6]\n\n# # Step 1: Calculate Distribution of Responses\n# frequency = df['Seat_Comfort'].value_counts(normalize=True).reindex(ordered_responses)\n# print(\"Frequency:\\n\", frequency)\n\n# # Step 2: Determine Cumulative Distribution\n# cumulative_distribution = frequency.cumsum()\n# print(\"Cumulative Distribution:\\n\", cumulative_distribution)\n\n# # Step 3: Assign Ratings Based on Cumulative Percentiles\n# # Map each response category to its cumulative distribution value\n# ratings = {response: cumulative_distribution[response] * 6 for response in cumulative_distribution.index}\n# print(\"Ratings Mapping:\\n\", ratings)\n","metadata":{"id":"bjy7bYhTncyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_age = df['Age'].mean()\n# df['Age'].fillna(mean_age, inplace=True)","metadata":{"id":"1ca60ecf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the 'Overall_Experience' column\noverall_experience = df['Overall_Experience']\ndf_without_overall_experience = df.drop(columns=['Overall_Experience'])\n\n# Initialize the IterativeImputer\nimputer = IterativeImputer(estimator=BayesianRidge(), random_state=0)\n\n# Fit the imputer on the dataset and transform it\nimputed_data = imputer.fit_transform(df_without_overall_experience)\n\n# Convert the imputed data back to a DataFrame and set the index\ndf_imputed = pd.DataFrame(imputed_data, columns=df_without_overall_experience.columns, index=df_without_overall_experience.index)\n\n# Concatenate the imputed data with the 'Overall_Experience' column\ndf_final = pd.concat([df_imputed, overall_experience], axis=1)\n","metadata":{"id":"Lyukhxldfq4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Convert imputed categorical columns back to integers\n# for col in cat_cols:\n#     df_final[col] = df_final[col].round().astype(int)","metadata":{"id":"UQcYQqdX0tOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.info()","metadata":{"id":"ZUj1Zjm4gMN4","executionInfo":{"status":"ok","timestamp":1721689227389,"user_tz":240,"elapsed":205,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"8c24b5cf-8a5e-49bb-8830-9c3585b95a0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_backup2 = df.copy(deep=True)\ndf = df_final.copy(deep=True)","metadata":{"id":"wfWUPmvshLNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a new column -> total_delay\ndf['Total_Delay'] = (df['Departure_Delay_in_Mins']+ df['Arrival_Delay_in_Mins'])\n\n# #create a new vari  able : over_all rating -> total 14 columns so dividing by 14 to keep the rating scale same\n# df['Overall_Rating'] = round((df['Onboard_Wifi_Service']+ df['Arrival_Time_Convenient'] +\n# df['Ease_of_Online_Booking'] + df['Platform_Location'] + df['Catering']+ df['Online_Support'] + df['Ease_of_Online_Booking'] +\n# df['Seat_Comfort'] + df['Onboard_Entertainment'] + df['Online_Boarding'] + df['Legroom'] +\n# df['Baggage_Handling'] + df['CheckIn_Service'] + df['Onboard_Service'] + df['Cleanliness'])/14,1)\n\n","metadata":{"id":"Y1ly8lkPsym1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the Overall_Rating using the new rating columns\ndf['Overall_Rating'] = round((\n    df['Onboard_Wifi_Service_rating'] +\n    df['Arrival_Time_Convenient_rating'] +\n    df['Ease_of_Online_Booking_rating'] +\n    df['Platform_Location_rating'] +\n    df['Catering_rating'] +\n    df['Online_Support_rating'] +\n    df['Seat_Comfort_rating'] +\n    df['Onboard_Entertainment_rating'] +\n    df['Online_Boarding_rating'] +\n    df['Legroom_rating'] +\n    df['Baggage_Handling_rating'] +\n    df['CheckIn_Service_rating'] +\n    df['Onboard_Service_rating'] +\n    df['Cleanliness_rating']\n) / 14, 1)\n","metadata":{"id":"N433cjcys9UT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"9I3TJXhWtLMw","executionInfo":{"status":"ok","timestamp":1721689324802,"user_tz":240,"elapsed":188,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"9f2dfb14-1d18-4f78-8063-56b2635c7e5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation for numerical_columns\ndf_corr = df.corr()\n\n# Plot correlations as a heatmap\n\nfig, ax = plt.subplots(figsize=(20, 16))\nsns.heatmap(df_corr, vmin=-1, vmax=1, cmap='RdBu_r',\n            xticklabels=True, yticklabels=True, annot=True, fmt='.2f',\n            annot_kws={'size': 10}, cbar_kws={'label': 'correlation'}, ax=ax)\n\n# Rotate the tick labels for better readability\nplt.xticks(rotation=45, ha='right', fontsize=12)\nplt.yticks(fontsize=12)\n\n# Adjust the font size of the color bar\ncbar = ax.collections[0].colorbar\ncbar.ax.tick_params(labelsize=12)\n\nplt.show()","metadata":{"id":"0f6ab8d7","executionInfo":{"status":"ok","timestamp":1721689332071,"user_tz":240,"elapsed":3177,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"722c4f84-be03-492d-e932-b9e55fdf25fd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df_backup3.copy(deep=True)","metadata":{"id":"5f0heXaQX-U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # List of categorical columns\n# PCA_cols = ['Overall_Rating','Onboard_Wifi_Service_rating','Arrival_Time_Convenient_rating','Ease_of_Online_Booking_rating','Platform_Location_rating',\n#     'Catering_rating', 'Online_Support_rating','Seat_Comfort_rating','Onboard_Entertainment_rating','Online_Boarding_rating','Legroom_rating',\n#     'Baggage_Handling_rating','CheckIn_Service_rating','Onboard_Service_rating','Cleanliness_rating']\n\n# # Extract the features (categorical fields) and target\n# X_cat = df[PCA_cols]\n\n# # Standardize the features\n# scaler = StandardScaler()\n# X_cat_scaled = scaler.fit_transform(X_cat)\n\n# # Apply PCA\n# pca = PCA(n_components=13)\n# X_cat_pca = pca.fit_transform(X_cat_scaled)\n# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n# print(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\n\n# X_cat_pca_df = pd.DataFrame(X_cat_pca, columns=[f'PC{i+1}' for i in range(X_cat_pca.shape[1])], index=df.index)\n\n# df_with_pca = df.join(X_cat_pca_df)","metadata":{"id":"PivgCJLKP0Ux","executionInfo":{"status":"ok","timestamp":1719453550404,"user_tz":240,"elapsed":168,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"4196e9af-2621-4c8a-c0e9-2e2efe61dace"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_with_pca_dropped = df_with_pca.drop(columns=PCA_cols)","metadata":{"id":"HFlzvFVnVZCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Correlation for numerical_columns\n# df_corr = df_with_pca_dropped.corr()\n\n# # Plot correlations as a heatmap\n\n# fig, ax = plt.subplots(figsize=(20, 16))\n# sns.heatmap(df_corr, vmin=-1, vmax=1, cmap='RdBu_r',\n#             xticklabels=True, yticklabels=True, annot=True, fmt='.2f',\n#             annot_kws={'size': 10}, cbar_kws={'label': 'correlation'}, ax=ax)\n\n# # Rotate the tick labels for better readability\n# plt.xticks(rotation=45, ha='right', fontsize=12)\n# plt.yticks(fontsize=12)\n\n# # Adjust the font size of the color bar\n# cbar = ax.collections[0].colorbar\n# cbar.ax.tick_params(labelsize=12)\n\n# plt.show()","metadata":{"executionInfo":{"status":"ok","timestamp":1719453555769,"user_tz":240,"elapsed":1906,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"44e4dadf-3363-4c1a-d9b9-27043f525000","id":"zvYV4wS7UuXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_backup3 = df.copy(deep=True)\n# df = df_with_pca_dropped.copy(deep=True)","metadata":{"id":"0d9MUdZlV0HT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Prepare train set and test set**","metadata":{"id":"uItUl73Nsv8p"}},{"cell_type":"code","source":"# Prepare Train set and test set\nX_train = df[df['Overall_Experience'].notnull()].drop(columns=['Overall_Experience'])\ny_train = df[df['Overall_Experience'].notnull()][['Overall_Experience']]\nX_test = df[df['Overall_Experience'].isnull()].drop(columns=['Overall_Experience'])\ny_test = df[df['Overall_Experience'].isnull()][['Overall_Experience']]","metadata":{"id":"rd9UdVWJDHAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Prepare Train set and test set\n# X_train2 = df[df['Overall_Experience'].notnull()].drop(columns=['Overall_Experience'])\n# y_train2 = df[df['Overall_Experience'].notnull()][['Overall_Experience']]\n# X_test2 = df[df['Overall_Experience'].isnull()].drop(columns=['Overall_Experience'])\n# y_test2 = df[df['Overall_Experience'].isnull()][['Overall_Experience']]","metadata":{"id":"7W7n2QovwK-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.astype('int')","metadata":{"id":"3bWDYMRgRTln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train2 = y_train2.astype('int')","metadata":{"id":"qVIMlb3UwU_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport re\n\n\n# Define a output directory in Google Drive\noutput_dir = '/content/drive/My Drive/ColabOutputs'\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the directory containing the selected CSV files\ndirectory = '/content/drive/My Drive/ColabOutputs/pred'","metadata":{"id":"oRVGC7fQwj5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(normalize=True)","metadata":{"id":"IyFmwJswaMBJ","executionInfo":{"status":"ok","timestamp":1721690004995,"user_tz":240,"elapsed":166,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"49f66445-11cd-46ac-82ca-a6430e0950cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.info()","metadata":{"id":"7gB6uqYfGvVh","executionInfo":{"status":"ok","timestamp":1721690112678,"user_tz":240,"elapsed":186,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"e3d71720-41a1-44d5-ea7c-e4ce145f5170"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"id":"3LbcFEcXZr8y","outputId":"9d816805-2c67-49ca-faf1-cbeb66c47d4e","executionInfo":{"status":"ok","timestamp":1721690116835,"user_tz":240,"elapsed":196,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building various models","metadata":{"id":"9VThYg7voGIz"}},{"cell_type":"code","source":"# Algorithms to use\nfrom sklearn import tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Metrics to evaluate the model\nfrom sklearn import metrics\n\nfrom sklearn.metrics import confusion_matrix, classification_report,recall_score,precision_score, accuracy_score\n\n# For tuning the model\nfrom sklearn.model_selection import GridSearchCV\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"id":"fswNIQwkbrT1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree\n  ","metadata":{"id":"zao_wOMvbRVy"}},{"cell_type":"code","source":"# Building decision tree model\ndt = DecisionTreeClassifier(class_weight = {0: 0.453, 1: 0.547}, random_state = 1)","metadata":{"id":"eae210cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting decision tree model\ndt.fit(X_train, y_train)","metadata":{"id":"c8a7c09b","outputId":"9b8e8648-7611-4ec0-e2de-714e916d4356","executionInfo":{"status":"ok","timestamp":1721691480353,"user_tz":240,"elapsed":1378,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check the model performance of decision tree**","metadata":{"id":"71330ff4"}},{"cell_type":"code","source":"# Checking performance on the training dataset\ny_train_pred_dt = dt.predict(X_train)\n\ndt_accuracy = accuracy_score(y_train, y_train_pred_dt)\nprint(f\"Decision Tree Test Accuracy: {dt_accuracy:.3f}\")\n","metadata":{"id":"b956e80d","outputId":"1deea0d7-f423-49ce-817d-145581fc6fbf","executionInfo":{"status":"ok","timestamp":1721691483235,"user_tz":240,"elapsed":203,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n- The Decision tree is giving a **100% score for all metrics on the training dataset.**","metadata":{"id":"92722b4e"}},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = dt.predict(X_test)\n\ny_test_pred_dt = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"1dee9f5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_dt.csv\"\ny_test_pred_dt.to_csv(filename, index=True)\n\n# Submitted to Hackathon and Returned Accuracy rate as 0.9309028","metadata":{"id":"2_4Fp27O4M0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot the feature importance and check the most important features.**","metadata":{"id":"6d944185"}},{"cell_type":"code","source":"# Plot the feature importance\n\nimportances = dt.feature_importances_\n\ncolumns = X_train.columns\n\nimportance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)\n\nplt.figure(figsize = (13, 13))\n\nsns.barplot(x=importance_df.Importance,y=importance_df.index);","metadata":{"id":"e48be259","outputId":"d2c12c07-d0ce-4526-a403-333b07dbfa40","executionInfo":{"status":"ok","timestamp":1721691626405,"user_tz":240,"elapsed":790,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = list(X_train.columns)\n\nplt.figure(figsize = (30, 20))\n\ntree.plot_tree(dt, max_depth = 4, feature_names = features, filled = True, fontsize = 12, node_ids = True, class_names = True)\n\nplt.show()","metadata":{"id":"da741f80","outputId":"a9bcf682-2834-43d1-8ecc-3cf8448698d1","executionInfo":{"status":"ok","timestamp":1721691655285,"user_tz":240,"elapsed":2592,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Building the Random Forest Classifier**","metadata":{"id":"-aa9K8H4kzNA"}},{"cell_type":"code","source":"# Fitting the Random Forest classifier on the training data\nrf_estimator = RandomForestClassifier(class_weight = {0: 0.453, 1: 0.547}, random_state = 1)\n\nrf_estimator.fit(X_train, y_train)","metadata":{"id":"2da36045","outputId":"df7b5c36-1474-4a60-ba63-9eed2871c1ab","executionInfo":{"status":"ok","timestamp":1721692870013,"user_tz":240,"elapsed":16656,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the training data\ny_pred_train_rf = rf_estimator.predict(X_train)\n\nrf_accuracy = accuracy_score(y_train, y_pred_train_rf)\nprint(f\"Decision Tree Test Accuracy: {rf_accuracy:.3f}\")","metadata":{"id":"06c28967","outputId":"ebd0933f-3ea5-4750-c177-7a1d5f2a5524","executionInfo":{"status":"ok","timestamp":1721692875789,"user_tz":240,"elapsed":1749,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\n- The Random Forest is giving a **100% score for all metrics on the training dataset.**","metadata":{"id":"de67259e"}},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = rf_estimator.predict(X_test)\n\ny_test_pred_rf = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"XYR9EKgglzAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_rf.csv\"\ny_test_pred_rf.to_csv(filename, index=True)\n\n#0.9511544","metadata":{"id":"TnWYNNb44yai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check the feature importance of the Random Forest**","metadata":{"id":"a378c849"}},{"cell_type":"code","source":"importances = rf_estimator.feature_importances_\n\ncolumns = X_train.columns\n\nimportance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)\n\nplt.figure(figsize = (13, 13))\n\nsns.barplot(x= importance_df.Importance, y=importance_df.index);","metadata":{"id":"4a672182","outputId":"72ad831e-2dfd-4ea0-ceae-35f631c85061"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Tuning the Random Forest classifier**","metadata":{"id":"ea991cf4"}},{"cell_type":"markdown","source":"**n_estimators**: The number of trees in the forest.\n\n**min_samples_split**: The minimum number of samples required to split an internal node.\n\n**min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n\n**max_features{“auto”, “sqrt”, “log2”, 'None'}**: The number of features to consider when looking for the best split.\n\n- If “auto”, then max_features=sqrt(n_features).\n\n- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n\n- If “log2”, then max_features=log2(n_features).\n\n- If None, then max_features=n_features.","metadata":{"id":"73e799cf"}},{"cell_type":"code","source":"# Choose the type of classifier\nrf_estimator_tuned = RandomForestClassifier(class_weight = {0: 0.453, 1: 0.547}, random_state = 1)\n\n# Grid of parameters to choose from\nparams_rf = {\n        \"n_estimators\": [100, 250, 500],\n        \"min_samples_leaf\": np.arange(1, 4, 1),\n        \"max_features\": [0.7, 0.9, 'auto'],\n}\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_estimator_tuned, params_rf, scoring = 'accuracy', cv = 5, n_jobs=-1)\n\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the classifier to the best combination of parameters\nrf_estimator_tuned = grid_obj.best_estimator_","metadata":{"id":"aef4a3ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_estimator_tuned.fit(X_train, y_train)","metadata":{"id":"2990168b","executionInfo":{"status":"ok","timestamp":1719411481213,"user_tz":240,"elapsed":195749,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"4593a409-6aae-426b-e0c8-0e9597ccfb9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the training data\ny_pred_train_rf_tuned = rf_estimator_tuned.predict(X_train)\n\nrf_tuned_accuracy = accuracy_score(y_train, y_pred_train_rf_tuned)\nprint(f\"Decision Tree Test Accuracy: {rf_tuned_accuracy:.3f}\")","metadata":{"id":"32bf508a","executionInfo":{"status":"ok","timestamp":1719411488081,"user_tz":240,"elapsed":6875,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"d822d99e-b9ff-441d-b7a8-71487de540d9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = rf_estimator_tuned.predict(X_test)\n\ny_pred_test_rf_tuned = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"lCQDVz6ip1iF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_rf_tuned.csv\"\ny_pred_test_rf_tuned.to_csv(filename, index=True)\n\n#0.9533453","metadata":{"id":"sF8LMC4cp1iO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **XGBoost**\n- XGBoost stands for Extreme Gradient Boosting.\n- XGBoost is a tree-based ensemble machine learning technique that improves prediction power and performance by improvising on the Gradient Boosting framework and incorporating reliable approximation algorithms. It is widely utilized and routinely appears at the top of competition leader boards in data science.","metadata":{"id":"oE3X_g38TZJZ"}},{"cell_type":"code","source":"# Installing the xgboost library using the 'pip' command.\n!pip install xgboost","metadata":{"id":"HsBUxH28TZJZ","outputId":"87478d38-89f0-474c-84c4-d4cb06a790a5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the AdaBoostClassifier and GradientBoostingClassifier [Boosting]\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n\n# Importing the XGBClassifier from the xgboost library\nfrom xgboost import XGBClassifier","metadata":{"id":"PAoSEdcfTZJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adaboost Classifier\nadaboost_model = AdaBoostClassifier(random_state = 1)\n\n# Fitting the model\nadaboost_model.fit(X_train, y_train)\n\n","metadata":{"id":"jErJULRITZJZ","outputId":"60a4571f-deb7-4edb-dcc3-51cf774798df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = adaboost_model.predict(X_test)\n\ny_test_pred_ada = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\n\n","metadata":{"id":"BZG_sw3yVNKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_ada.csv\"\ny_test_pred_ada.to_csv(filename, index=True)","metadata":{"id":"3sTZPkYAVNKu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boost Classifier\ngbc = GradientBoostingClassifier(random_state = 1)\n\n# Fitting the model\ngbc.fit(X_train, y_train)\n\n","metadata":{"id":"G4qxprvfTZJZ","outputId":"7a4b06f7-8857-4ba6-94f7-9245cd56dbc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = gbc.predict(X_test)\n\ny_test_pred_gbc = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"hW8h-1fkV3xT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_gbc.csv\"\ny_test_pred_gbc.to_csv(filename, index=True)","metadata":{"id":"VTAl25rUV3xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost Classifier\nxgb = XGBClassifier(random_state = 1, eval_metric = 'logloss')\n\n# Fitting the model\nxgb.fit(X_train,y_train)\n\n","metadata":{"id":"ixUBd2kGTZJZ","outputId":"ebe7937d-6a12-4c51-91a5-e5e3b6f6d81f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = xgb.predict(X_test)\n\ny_test_pred_xgb = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\n\n#0.9534858","metadata":{"id":"qkxZJKjoWKqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_xgb.csv\"\ny_test_pred_xgb.to_csv(filename, index=True)","metadata":{"id":"80DhB9prWKqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Hyperparameter Tuning**","metadata":{"id":"dCh2oKzLRM3a"}},{"cell_type":"code","source":"# Define the model\nxgb_tuned = XGBClassifier(random_state=1, eval_metric='logloss')\n\n# Define the model with GPU support\nxgb_gpu = XGBClassifier(\n    random_state=1,\n    eval_metric='logloss',\n    tree_method='gpu_hist'\n)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0],\n    'min_child_weight': [1, 3, 5],\n    'gamma': [0, 0.1, 0.2],\n    'reg_alpha': [0, 0.1, 1],\n    'reg_lambda': [1, 2, 5],\n    'booster': ['gbtree', 'gblinear']\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=1)\n\n# Define the GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = grid_search.best_params_\nxgb_tuned = XGBClassifier(random_state=1, eval_metric='logloss', tree_method='gpu_hist', **best_params)\nxgb_tuned.fit(X_train, y_train)","metadata":{"id":"YoFuL07lXPVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = xgb_tuned.predict(X_test)\n\ny_test_pred_xgb_tuned = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\n\n","metadata":{"id":"4C32OnNkZaL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_xgb_tuned.csv\"\ny_test_pred_xgb_tuned.to_csv(filename, index=True)","metadata":{"id":"bThJV7quZaMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\ngbc_tuned = GradientBoostingClassifier(random_state=1)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.8, 0.9, 1.0],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'loss': ['deviance', 'exponential']  # Adding loss to the grid\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Define the GridSearchCV\ngrid_search = GridSearchCV(estimator=gbc_tuned, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = grid_search.best_params_\ngbc_tuned = GradientBoostingClassifier(random_state=1, **best_params)\ngbc_tuned.fit(X_train, y_train)\n\n","metadata":{"id":"rtotCWlnaPiM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = gbc_tuned.predict(X_test)\n\ny_test_pred_gbc_tuned = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"Ypv1PCSPanFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_gbc_tuned.csv\"\ny_test_pred_gbc_tuned.to_csv(filename, index=True)","metadata":{"id":"0TbG0842anF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nadaboost_tuned = AdaBoostClassifier(random_state=1)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'learning_rate': [0.01, 0.1, 1.0],\n    'base_estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=3)]\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Define the GridSearchCV\ngrid_search = GridSearchCV(estimator=adaboost_tuned, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=-1)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = grid_search.best_params_\nadaboost_tuned = AdaBoostClassifier(random_state=1, **best_params)\nadaboost_tuned.fit(X_train, y_train)","metadata":{"id":"GbO1pS6UbVOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking performance on the test dataset\npredictions = adaboost_tuned.predict(X_test)\n\ny_test_pred_ada_tuned = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])","metadata":{"id":"p9KsnKhxbkf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = f\"{output_dir}/Submission_ada_tuned.csv\"\ny_test_pred_ada_tuned.to_csv(filename, index=True)","metadata":{"id":"PuG7HYobbkgB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Other classifiers**","metadata":{"id":"NMa_ouVbRjjS"}},{"cell_type":"code","source":"!pip install hyperopt","metadata":{"id":"9a8f3b07","outputId":"d64cb0db-28df-48d3-d2b9-b4b8c6180717"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom hyperopt import fmin, tpe, hp, Trials\nfrom sklearn.feature_selection import RFE","metadata":{"id":"b3809f0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'bagging_temperature': 0.5,\n    'border_count': 50,\n    'depth': 6,\n    'iterations': 300,\n    'l2_leaf_reg': 1,\n    'learning_rate': 0.1,\n    'random_strength': 2\n}\n","metadata":{"id":"2flaVhZTqNH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [\n     ('Logistic Regression', LogisticRegression(solver='liblinear')),\n    ('K-Nearest Neighbors', KNeighborsClassifier()),\n    ('Decision Tree', DecisionTreeClassifier()),\n    ('Random Forest', RandomForestClassifier(class_weight={0: 0.453, 1: 0.547},\n                       n_estimators=100)),\n    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n    ('Gaussian Naive Bayes', GaussianNB()),\n    ('Multinomial Naive Bayes', MultinomialNB()),\n    ('Bernoulli Naive Bayes', BernoulliNB()),\n    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n    ('LightGBM', LGBMClassifier(random_state=42)),\n    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n    ('Ridge', RidgeClassifier(random_state=42)),\n    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state=42))\n\n]\n\n# Iterate through the classifiers, fit, and generate predition on test set\nfor name, clf in classifiers:\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    y_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\n    filename = f\"{output_dir}/Sub_{name}.csv\"\n    y_test_pred.to_csv(filename, index=True)","metadata":{"id":"ea7c73c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hybrid Model - Majority Voting","metadata":{"id":"Xr2zYyJGTH7l"}},{"cell_type":"code","source":"hybrid_train_df = pd.DataFrame()\nhybrid_test_df = pd.DataFrame()","metadata":{"id":"vwv07e4xyiWE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, clf in classifiers:\n    predictions = clf.predict(X_train)\n    y_train_pred = pd.DataFrame(predictions, index=X_train.index, columns=[f\"Overall_Experience_1_{name}\"])\n    # Concatenate this frame to the combined_df\n    if hybrid_train_df.empty:\n        hybrid_train_df = y_train_pred  # If empty, initialize it with the first df\n    else:\n        hybrid_train_df = pd.concat([hybrid_train_df, y_train_pred], axis=1)\n\n    predictions = clf.predict(X_test)\n    y_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=[f\"Overall_Experience_1_{name}\"])\n    # Concatenate this frame to the combined_df\n    if hybrid_test_df.empty:\n        hybrid_test_df = y_test_pred  # If empty, initialize it with the first df\n    else:\n        hybrid_test_df = pd.concat([hybrid_test_df, y_test_pred], axis=1)\n\n\n\n","metadata":{"id":"yYn53q3AVk0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hybrid_test_df['Hybrid_Prediction'] = hybrid_test_df.mode(axis=1)[0].astype(int)","metadata":{"id":"vE1OXLM_DICL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hybrid_test_df['Hybrid_Prediction']","metadata":{"id":"YcrigZR4FJiV","executionInfo":{"status":"ok","timestamp":1719464693671,"user_tz":240,"elapsed":138,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"bb62d872-767e-4d26-8290-d8efe066f22f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = pd.DataFrame(hybrid_test_df['Hybrid_Prediction'], index=hybrid_test_df.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_majorhybrid.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"VQnCTozJEjBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Other Algorithms","metadata":{"id":"fYoY0OBVYwe4"}},{"cell_type":"code","source":"# Importing libraries for building linear regression model\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.decomposition import PCA\n","metadata":{"id":"Js27umrUvbn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_h = hybrid_train_df.copy(deep=True)\ny_train_h = y_train.copy(deep=True)\nX_test_h = hybrid_test_df.copy(deep=True)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_h_scaled = scaler.fit_transform(X_train_h)\nX_test_h_scaled = scaler.transform(X_test_h)\n\n# Handle correlation with PCA\npca = PCA(n_components=4)\nX_train_h_pca = pca.fit_transform(X_train_h_scaled)\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\nX_test_h_pca = pca.transform(X_test_h_scaled)","metadata":{"id":"MJi7GXWBTEUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred.info()","metadata":{"id":"VrBxrI9-dm49","executionInfo":{"status":"ok","timestamp":1719462293417,"user_tz":240,"elapsed":137,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"3a7e3a8c-9af9-439d-e5e8-45a70ca63d5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train a linear regression model using the PCA components\nlr = LinearRegression()\nlr.fit(X_train_h, y_train_h)\npredictions = lr.predict(X_test_h)\nprint(predictions)\nbinary_predictions = (predictions >= 0.5).astype(int)\nprint(binary_predictions)\ny_test_pred = pd.DataFrame(binary_predictions, index=X_test_h.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_lr_hybrid3.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"2iWllZumcO2W","executionInfo":{"status":"ok","timestamp":1719463304639,"user_tz":240,"elapsed":143,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"b11d43fa-f644-4a7a-bd84-9d018e559a12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [\n     ('Logistic Regression', LogisticRegression(solver='liblinear')),\n    ('K-Nearest Neighbors', KNeighborsClassifier()),\n    ('Decision Tree', DecisionTreeClassifier()),\n    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n    ('Gaussian Naive Bayes', GaussianNB()),\n    ('Multinomial Naive Bayes', MultinomialNB()),\n    ('Bernoulli Naive Bayes', BernoulliNB()),\n    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n    ('LightGBM', LGBMClassifier(random_state=42)),\n    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n    ('Ridge', RidgeClassifier(random_state=42)),\n    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state=42))\n\n]\n\n# Iterate through the classifiers\nfor name, clf in classifiers:\n    clf.fit(X_train_h, y_train)\n\n    # predictions = clf.predict(X_test_h)\n    # binary_predictions = (predictions >= 0.5).astype(int)\n\n    # Handle probability outputs if applicable\n    if hasattr(clf, \"predict_proba\"):\n        predictions = clf.predict_proba(X_test_h)[:, 1]\n        binary_predictions = (predictions >= 0.5).astype(int)\n    else:\n        binary_predictions = clf.predict(X_test_h)\n\n    y_test_pred = pd.DataFrame(binary_predictions, index=X_test_h.index, columns=['Overall_Experience'])\n    filename = f\"{output_dir}/Sub_10_h4_{name}.csv\"\n    y_test_pred.to_csv(filename, index=True)","metadata":{"id":"42ML-XsurRLb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection","metadata":{"id":"PHTLRe01Y7aR"}},{"cell_type":"code","source":"# Get the feature importances\nfeature_importances = clf.feature_importances_\n\n# Print the feature importances\nfor feature, importance in zip(X_train.columns, feature_importances):\n    print(f'Feature: {feature}, Importance: {importance}')","metadata":{"id":"ip-YEWakR15T","executionInfo":{"status":"ok","timestamp":1719283725294,"user_tz":240,"elapsed":362,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"b8725efc-2716-4ce8-f51a-af083a007cb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(X_train.columns, feature_importances)\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances')\nplt.show()","metadata":{"id":"x1XaTwLjTAx3","executionInfo":{"status":"ok","timestamp":1719283753861,"user_tz":240,"elapsed":723,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"cd2ef6e5-3a51-4b07-9f88-b093ac2025a0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sel = X_train.drop(['Total_Delay', 'Seat_Class', 'Departure_Delay_in_Mins'], axis=1)\nX_test_sel = X_test.drop(['Total_Delay', 'Seat_Class', 'Departure_Delay_in_Mins'], axis=1)","metadata":{"id":"d8LxKx4_UAWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sel = X_train.drop(['Total_Delay'], axis=1)\nX_test_sel = X_test.drop(['Total_Delay'], axis=1)","metadata":{"id":"_U60-MmRXQFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features_model = CatBoostClassifier(verbose=0, random_state=42)\nselected_features_model.fit(X_train_sel, y_train)\npredictions = selected_features_model.predict(X_test_sel)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_cat_sel.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"QTeDDDXfUy5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.info()","metadata":{"id":"pR5kjNpJUNZE","executionInfo":{"status":"ok","timestamp":1719284071678,"user_tz":240,"elapsed":537,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"3d4a729c-c1c2-4f60-b173-8459f615ce8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the model's parameters after fitting\nparams = clf.get_params()\n\n# Print the parameters\nfor param, value in params.items():\n    print(f'{param}: {value}')","metadata":{"id":"jhZCjkGJSR4z","executionInfo":{"status":"ok","timestamp":1719283671014,"user_tz":240,"elapsed":419,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"71fcb054-6533-4387-c71a-31218d75238d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install eli5","metadata":{"id":"jO_TqZjd5lGg","executionInfo":{"status":"ok","timestamp":1721751682814,"user_tz":240,"elapsed":7723,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"19942104-642f-4322-e5f9-d8a10529b974"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for negative values in the dataset\nnegative_values = (X_train < 0).any()\nprint(\"Features containing negative values:\")\nprint(negative_values[negative_values])","metadata":{"id":"dPBC8cgUzFob","executionInfo":{"status":"ok","timestamp":1719283834713,"user_tz":240,"elapsed":371,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"2e3935d3-f36c-4e62-cdbe-81aa471d8a74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\n\n\n# Apply Chi-Squared test\nk = 3  # Number of top features to select\nchi2_selector = SelectKBest(chi2, k=k)\nX_train_kbest = chi2_selector.fit_transform(X_train, y_train)\nX_test_kbest = chi2_selector.transform(X_test)\n\n# Train a model using the selected features\nselected_features_model = CatBoostClassifier(verbose=0, random_state=42)\nselected_features_model.fit(X_train_kbest, y_train)\npredictions = selected_features_model.predict(X_test_kbest)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_cat_chi.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"65tKo890x2DY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Compute permutation importance using the validation set\nperm = PermutationImportance(clf, random_state=42).fit(X_train, y_train)\n\n# Display permutation importance\neli5.show_weights(perm, feature_names=X_train.columns.tolist())","metadata":{"id":"DgprLv9w4fEU","executionInfo":{"status":"ok","timestamp":1719277108352,"user_tz":240,"elapsed":8414,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"15fde530-a62d-480d-ae12-1c603160b46d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = X_train.columns.tolist()\n# Select important features\nthreshold = 0.01  # Define a threshold for feature importance\nimportant_features = [feature for feature, importance in zip(feature_names, perm.feature_importances_) if importance > threshold]\nprint(f\"Important features: {important_features}\")\n\n# Retrain model with important features\nX_train_important = X_train[important_features]\nX_test_important = X_test[important_features]\nclf.fit(X_train_important, y_train)\n\n# Evaluate new model\npredictions = clf.predict(X_test_important)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_Cat_PerImportant.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"r14IrJRa579E","executionInfo":{"status":"ok","timestamp":1719278175144,"user_tz":240,"elapsed":7483,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"9097b110-76f7-4875-a98f-c51df60d7bd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# Define the output directory in Google Drive\noutput_dir = '/content/drive/My Drive/ColabOutputs'\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the model\ncat_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0')\n\n# Define RFE\nrfe = RFE(estimator=cat_model, n_features_to_select=20)\n\n# Fit RFE on the training data\nrfe.fit(X_train, y_train)\n\n# Transform the training and testing sets\nX_train_rfe = rfe.transform(X_train)\nX_test_rfe = rfe.transform(X_test)\n\n# Train the model on the selected features\ncat_model.fit(X_train_rfe, y_train)\n\n# Make predictions on the testing set\npredictions = cat_model.predict(X_test_rfe)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_8_CatBoost_RFE_GPU.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"M-I2-qGqfmBw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Hyperparameter tuning\n\nTry different approachs to speed up the hyperparameter tuning process, making it more efficient to find the best model configuration.\n\n\n*   GPU support\n*   Early stopping\n*   RandomizedSearchCV\n*   Dask's parallelized RandomizedSearchCV\n\n\n","metadata":{"id":"c6Tj4-9WaCYv"}},{"cell_type":"code","source":"!pip install dask-ml","metadata":{"id":"URyyIAUJTM_J","executionInfo":{"status":"ok","timestamp":1719314923800,"user_tz":240,"elapsed":7323,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"96fa7d4c-669c-4cb9-fe1b-29b0ee4cb2f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\nfrom dask_ml.model_selection import RandomizedSearchCV as DaskRandomizedSearchCV\nimport dask.dataframe as dd\n\n# Define the output directory in Google Drive\noutput_dir = '/content/drive/My Drive/ColabOutputs'\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert pandas DataFrame to Dask DataFrame\nX_train_dask = dd.from_pandas(X_train, npartitions=2)\ny_train_dask = dd.from_pandas(y_train, npartitions=2)\n\n# Define the CatBoost model\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10)\n\n# Define the parameter grid\nparam_grid = {\n    'iterations': [100, 200, 300, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [4, 5, 6, 7],\n    'l2_leaf_reg': [1, 3, 5, 7],\n    'border_count': [32, 50, 100],\n    'bagging_temperature': [0.5, 1, 1.5],\n    'random_strength': [1, 2, 3]\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n\n# Define the DaskRandomizedSearchCV\nrandom_search = DaskRandomizedSearchCV(estimator=catboost_model, param_distributions=param_grid, scoring='accuracy', cv=cv, n_jobs=1, n_iter=50, random_state=1)\n\n# Fit the RandomizedSearchCV\nrandom_search.fit(X_train_dask, y_train_dask)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", random_search.best_params_)\nprint(\"Best Score:\", random_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = random_search.best_params_\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10, **best_params)\ncatboost_model.fit(X_train, y_train)\n\n# Predict for X_test\npredictions = catboost_model.predict(X_test)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_CatBoost_Tuned_GPU.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"cLJy2SvQNfu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\nfrom catboost import CatBoostClassifier\n\n# Define the output directory in Google Drive\noutput_dir = '/content/drive/My Drive/ColabOutputs'\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the CatBoost model\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10)\n\n# Define the parameter grid\nparam_grid = {\n    'iterations': [100, 200, 300, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [4, 5, 6, 7],\n    'l2_leaf_reg': [1, 3, 5, 7],\n    'border_count': [32, 50, 100],\n    'bagging_temperature': [0.5, 1, 1.5],\n    'random_strength': [1, 2, 3]\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=1)\n\n# Define the RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=catboost_model, param_distributions=param_grid, scoring='accuracy', cv=cv, n_jobs=1, n_iter=50, random_state=1)\n\n# Fit the RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", random_search.best_params_)\nprint(\"Best Score:\", random_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = random_search.best_params_\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10, **best_params)\ncatboost_model.fit(X_train, y_train)\n\n# Predict for X_test\npredictions = catboost_model.predict(X_test)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_CatBoost_Tuned_GPU.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"shSAh3RnZeEU","executionInfo":{"status":"ok","timestamp":1719319599712,"user_tz":240,"elapsed":355996,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"c7da3074-8b89-44cd-b21b-1f5d30bb0cdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the CatBoost model\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10)\n\n# Define the parameter grid\nparam_grid = {\n    'iterations': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [3, 4, 5, 6],\n    'l2_leaf_reg': [1, 3, 5, 7],\n    'border_count': [32, 50, 100],\n    'bagging_temperature': [0.5, 1, 2],\n    'random_strength': [1, 2, 3]\n}\n\n# Define the cross-validation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# Define the GridSearchCV\ngrid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, scoring='accuracy', cv=cv, n_jobs=1)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Output the best parameters and the best score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\n# Train the final model using the best parameters\nbest_params = grid_search.best_params_\ncatboost_model = CatBoostClassifier(verbose=0, random_state=42, task_type='GPU', devices='0', early_stopping_rounds=10, **best_params)\ncatboost_model.fit(X_train, y_train)\n\n# Predict for X_test\npredictions = catboost_model.predict(X_test)\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_9_CatBoost_Tuned_GPU.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"3PQjzt0ojKue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hybrid Model - accuracy extraction\n\nThe submission files have been renamed with their accuracy rate returned by Hackathon. Extract the accuracy for later use.  ","metadata":{"id":"qaYrQqMapeco"}},{"cell_type":"code","source":"# # Read the CSV file into a DataFrame\n# df_1 = pd.read_csv('/content/drive/MyDrive/ColabOutputs/Sub_9_CatBoost_time_non_immute(0.9564912).csv')\n# df_2 = pd.read_csv('/content/drive/MyDrive/ColabOutputs/Sub_9_CatBoost_age_immuted(0.956435).csv')\n# df_3 = pd.read_csv('/content/drive/MyDrive/ColabOutputs/Sub_6_XGBoost(0.9551991).csv')\n\n\n# # Convert DataFrame columns to arrays\n# pred1 = df_1['Overall_Experience'].values\n# pred2 = df_2['Overall_Experience'].values\n# #pred3 = df_3['Overall_Experience'].values\n\n# # Sample data\n# pred = [pred1, pred2, pred3]  # List of arrays with predictions\n# accuracies = [0.9564912, 0.956435, 0.9551991]  # Corresponding accuracies of the predictions\n\n# # Convert accuracies to weights\n# weights = np.array(accuracies) / np.sum(accuracies)\n\n# # Compute weighted average of predictions\n# y_test_estimated = np.average(pred, axis=0, weights=weights)\n\n# # round the estimated values\n# predictions = np.round(y_test_estimated).astype(int)\n\n# y_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\n# filename = f\"{output_dir}/Sub_9_hybrid.csv\"\n# y_test_pred.to_csv(filename, index=True)","metadata":{"id":"i_yPvSd0aKsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import glob\n# import os\n# import re\n# import numpy as np\n# import pandas as pd\n\n# # Define a directory in your Google Drive\n# output_dir = '/content/drive/My Drive/ColabOutputs'\n# os.makedirs(output_dir, exist_ok=True)\n\n# # Define the directory containing the CSV files\n# directory = '/content/drive/My Drive/ColabOutputs/pred'\n\n# List all CSV files in the directory\n#csv_files = glob.glob(os.path.join(directory, \"Sub*.csv\"))\ncsv_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\nprint(csv_files)\n\n# Create a dictionary to store numpy arrays and a list for accuracies\narrays = {}\naccuracies = []\ndataframes = []\n\n# Regular expression pattern to extract accuracy values from filenames\npattern = re.compile(r\"Sub.*\\(([\\d.]+)\\)\\.csv\")\n\n# Read each CSV file into a DataFrame, then convert to numpy array, and store it in the dictionary\nfor i, file in enumerate(csv_files, start=1):\n    # Extract accuracy value from the filename\n\n    match = pattern.search(file)\n    if match:\n        accuracy = float(match.group(1))\n        accuracies.append(accuracy)\n        print(accuracy)\n        # Read the CSV file into a DataFrame\n        pred = pd.read_csv(file, usecols=[\"Overall_Experience\"])\n        # Convert the DataFrame to a numpy array\n        arrays[f'df_{i}'] = pred[\"Overall_Experience\"].values\n        pred2 = pd.read_csv(file, usecols=[\"ID\", \"Overall_Experience\"])\n        pred2 = pred2.set_index(\"ID\")\n\n        dataframes.append(pred2.rename(columns={\"Overall_Experience\": f\"Overall_Experience_{i}\"}))\n\n# Convert the dictionary to a list of arrays\narray_list = list(arrays.values())\n\n# Convert accuracies to a numpy array\naccuracies = np.array(accuracies)\n\n# Combine all DataFrames into a single DataFrame\ncombined_df = pd.concat(dataframes, axis=1)\n\n\n# Display the loaded arrays and accuracies\nfor i, array in enumerate(array_list, start=1):\n    print(f\"Array {i}:\")\n    print(array[:5])  # Display the first few rows of each array\n\nprint(f\"Accuracies: {accuracies}\")","metadata":{"id":"xBE6tBGQfC66","executionInfo":{"status":"ok","timestamp":1719470079586,"user_tz":240,"elapsed":528,"user":{"displayName":"Sophia Liang","userId":"06939372482789698041"}},"outputId":"89611ee5-8e48-4f8a-cc1c-7178ae57bce7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"id":"eFYlVswBapty","executionInfo":{"status":"ok","timestamp":1719369640981,"user_tz":240,"elapsed":578,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"b3ddca70-1b23-4eb7-802f-162aa378c4e3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Agreement and Disagreement Analysis\n\nAnalyzing model agreement and disagreement can provide valuable insights into how different models behave and where they tend to make similar or divergent predictions. This analysis helps in understanding the strengths and weaknesses of each model, identifying patterns, and potentially improving the hybrid model.","metadata":{"id":"BvRWLbtbtYIL"}},{"cell_type":"code","source":"# Function to check if all values in a row are the same\ndef not_all_equal(row):\n    return len(set(row)) > 1\n\n# Select rows where any column value is not the same\ndf_filtered = combined_df[combined_df.apply(not_all_equal, axis=1)]\n\nprint(df_filtered)","metadata":{"id":"7xv-h59zb6KB","executionInfo":{"status":"ok","timestamp":1719369984702,"user_tz":240,"elapsed":265,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"7fd65b07-eabd-4095-9355-5d90166a0cd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hybrid_df = pd.read_csv(\"/content/drive/My Drive/ColabOutputs/pred/hybrid (9575305).csv\", usecols=[\"ID\", \"Overall_Experience\"])\nhybrid_df = hybrid_df.set_index(\"ID\")","metadata":{"id":"5IoX236LfnvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.merge(hybrid_df, combined_df, on='ID', how='inner')\n\nprint(merged_df)","metadata":{"id":"31Gbvaalgjrn","executionInfo":{"status":"ok","timestamp":1719371227800,"user_tz":240,"elapsed":266,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"e371120f-28ea-4bb3-d525-1781eb715f91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns',100)","metadata":{"id":"Rcp-m8Mpd9jO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df[merged_df['Overall_Experience_4']!=merged_df['Overall_Experience']]","metadata":{"id":"MEAGpxcxgypx","executionInfo":{"status":"ok","timestamp":1719371383566,"user_tz":240,"elapsed":324,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"e6705547-34cc-40ba-9a1b-e392e04e6f4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_filtered","metadata":{"id":"H8-MSNMAd_O8","executionInfo":{"status":"ok","timestamp":1719374440795,"user_tz":240,"elapsed":399,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"64434aec-38fd-4333-f43a-3bc795aadff4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_X_test = X_test[X_test.index.isin(df_filtered.index)]","metadata":{"id":"MDaml24UoSkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_X_test","metadata":{"id":"2wHzQMW0pdxE","executionInfo":{"status":"ok","timestamp":1719373529007,"user_tz":240,"elapsed":260,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"95c0ef05-529e-4b06-a2a3-76cb2ff41880"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df_test = df_test[df_test['ID'].isin(df_filtered.index)]","metadata":{"id":"gnz0KvPtpsGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df_test","metadata":{"id":"oXwZI-1lpwov","executionInfo":{"status":"ok","timestamp":1719373860140,"user_tz":240,"elapsed":573,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"e338c223-0230-4907-8880-a09d9d3ed40a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of columns to be plotted\nnum_of_cat_cols = len(cat_cols)\n\n# Determine the number of rows and columns for the subplots\nn_cols = 4  # Number of columns in the grid\nn_rows = (num_of_cat_cols // n_cols) + (num_of_cat_cols % n_cols > 0)  # Calculate rows needed\n\n# Create subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 15))\naxes = axes.flatten()  # Flatten the array of axes for easy iteration\n\n# Loop through the categorical columns and create a bar plot for each\nfor i, col in enumerate(cat_cols):\n    sns.countplot(data=filtered_df_test, x=col, palette='viridis', ax=axes[i])\n    axes[i].set_title(f'Bar Plot of {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=90)\n\n# Remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()","metadata":{"id":"SkGkydwZrpkI","executionInfo":{"status":"ok","timestamp":1719374129476,"user_tz":240,"elapsed":4619,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"e5f82d7c-8fe5-4206-fc65-03a8db6dafe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Hybrid Model - Majority and Weighted Voting\n\n* Majority Voting\n* Use the accuracy as weight to do voting","metadata":{"id":"BXxox6lYvbn6"}},{"cell_type":"code","source":"# Convert accuracies to weights\nweights = np.array(accuracies) / np.sum(accuracies)\n\n# Compute weighted average of predictions\ny_test_estimated = np.average(array_list, axis=0, weights=weights)\n\n# round the estimated values\npredictions = np.round(y_test_estimated).astype(int)\n\n# Compute majority voting\nmajority_predictions = pd.DataFrame(array_list).mode(axis=0).iloc[0].astype(int).values\n\ny_test_pred = pd.DataFrame(predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_10_hybrid_w.csv\"\ny_test_pred.to_csv(filename, index=True)\n\ny_test_pred = pd.DataFrame(majority_predictions, index=X_test.index, columns=['Overall_Experience'])\nfilename = f\"{output_dir}/Sub_10_hybrid_m.csv\"\ny_test_pred.to_csv(filename, index=True)","metadata":{"id":"jwEkHCkFiF2i"},"execution_count":null,"outputs":[]}]}